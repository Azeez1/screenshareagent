<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dux Machina Live Assistant Demo</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap">
    <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-pink.min.css">
    <script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Inter', sans-serif;
        }

        body {
            background-color: #f5f5f7;
        }

        .mdl-layout__header {
            background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%);
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);
        }

        .mdl-layout-title {
            font-family: 'Inter', sans-serif;
            font-weight: 600;
            font-size: 1.25rem;
        }

        .demo-content {
            padding: 2rem;
            max-width: 1200px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 2rem;
        }

        .video-section {
            background: white;
            padding: 1.5rem;
            border-radius: 1rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);
        }

        .chat-section {
            background: white;
            padding: 1.5rem;
            border-radius: 1rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);
            display: flex;
            flex-direction: column;
        }

        #videoElement {
            width: 100%;
            height: auto;
            border-radius: 0.75rem;
            background: #f3f4f6;
        }

        #canvasElement {
            display: none;
        }

        .button-group {
            display: flex;
            gap: 1rem;
            margin-top: 1rem;
            justify-content: center;
        }

        .control-button {
            width: 48px;
            height: 48px;
            border-radius: 50%;
            border: none;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }

        #startButton {
            background: #10b981;
            color: white;
        }

        #startButton:hover {
            background: #059669;
            transform: scale(1.05);
        }

        #stopButton {
            background: #ef4444;
            color: white;
        }

        #stopButton:hover {
            background: #dc2626;
            transform: scale(1.05);
        }

        #chatLog {
            flex-grow: 1;
            overflow-y: auto;
            margin-top: 1rem;
            padding: 1rem;
            background: #f8fafc;
            border-radius: 0.5rem;
            height: 400px;
        }

        #chatLog p {
            margin-bottom: 0.75rem;
            padding: 0.75rem;
            border-radius: 0.5rem;
            background: white;
            box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1);
        }

        .section-title {
            font-size: 1.25rem;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 1rem;
        }

        #screenShareButton {
            background: #6366f1;
            color: white;
        }

        #screenShareButton:hover {
            background: #4f46e5;
            transform: scale(1.05);
        }

        #screenShareButton.active {
            background: #4f46e5;
            box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.3);
        }

        #micButton {
            background: #ef4444;
            color: white;
        }

        #micButton:hover {
            transform: scale(1.05);
        }

        #micButton.active {
            background: #10b981;
            box-shadow: 0 0 0 3px rgba(16, 185, 129, 0.3);
        }

        .status-indicator, .mic-status {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin-top: 0.5rem;
            justify-content: center;
            font-size: 0.875rem;
            color: #6b7280;
        }

        .status-dot, .mic-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #9ca3af;
        }

        .status-dot.active, .mic-dot.active {
            background: #10b981;
            box-shadow: 0 0 0 3px rgba(16, 185, 129, 0.2);
        }

        @media (max-width: 768px) {
            .demo-content {
                grid-template-columns: 1fr;
            }
        }

        .demo-info {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }

        .info-card {
            background: #f8fafc;
            padding: 1.25rem;
            border-radius: 0.75rem;
            box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1);
        }

        .info-card h3 {
            color: #1f2937;
            font-size: 1.125rem;
            font-weight: 600;
            margin-bottom: 0.75rem;
        }

        .info-card p {
            color: #4b5563;
            font-size: 0.875rem;
            line-height: 1.5;
        }

        .status-grid {
            display: grid;
            gap: 0.75rem;
        }

        .status-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 0.5rem;
            background: white;
            border-radius: 0.5rem;
        }

        .status-label {
            color: #6b7280;
            font-size: 0.875rem;
        }

        .status-value {
            color: #1f2937;
            font-weight: 500;
            font-size: 0.875rem;
        }
    </style>
</head>

<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
        <header class="mdl-layout__header">
            <div class="mdl-layout__header-row">
                <span class="mdl-layout-title">Dux Machina Live Assistant Demo</span>
            </div>
        </header>
        <main class="mdl-layout__content">
            <div class="page-content">
                <div class="demo-content">
                    <div class="video-section">
                        <h2 class="section-title">Screen Share</h2>
                        <div class="button-group">
                            <button id="screenShareButton" class="control-button">
                                <i class="material-icons">screen_share</i>
                            </button>
                        </div>
                        <div class="status-indicator">
                            <div class="status-dot"></div>
                            <span class="status-text">Screen sharing inactive</span>
                        </div>
                        <video id="videoElement" autoplay></video>
                        <canvas id="canvasElement"></canvas>
                        <div class="button-group">
                            <button id="micButton" class="control-button">
                                <i class="material-icons">mic_off</i>
                            </button>
                        </div>
                        <div class="mic-status">
                            <div class="mic-dot"></div>
                            <span class="mic-status-text">Microphone inactive</span>
                        </div>
                    </div>
                    <div class="chat-section">
                        <h2 class="section-title">Assistant Chat</h2>
                        <div id="chatLog"></div>
                    </div>
                </div>
            </div>
        </main>
    </div>

    <script defer>
        const URL = `ws://localhost:9083`;
        const video = document.getElementById("videoElement");
        const canvas = document.getElementById("canvasElement");
        let context;

        // Initialize context here
        window.addEventListener("load", () => {
            context = canvas.getContext("2d");
            setInterval(captureImage, 3000);
        });

        const screenShareButton = document.getElementById('screenShareButton');
        const statusDot = document.querySelector('.status-dot');
        const statusText = document.querySelector('.status-text');
        let isScreenSharing = false;

        const micButton = document.getElementById('micButton');
        const micDot = document.querySelector('.mic-dot');
        const micStatusText = document.querySelector('.mic-status-text');
        let isMicActive = false;

        let stream = null;
        let currentFrameB64;
        let webSocket = null;
        let audioContext = null;
        let mediaRecorder = null;
        let processor = null;
        let pcmData = [];
        let interval = null;
        let initialized = false;
        let audioInputContext;
        let workletNode;
        let audioStream = null;

        // Function to start screen capture
        async function startScreenShare() {
            try {
                stream = await navigator.mediaDevices.getDisplayMedia({
                    video: {
                        width: { max: 640 },
                        height: { max: 480 },
                    },
                });

                video.srcObject = stream;
                await new Promise(resolve => {
                    video.onloadedmetadata = () => {
                        console.log("video loaded metadata");
                        resolve();
                    }
                });

                // Update UI to show active state
                isScreenSharing = true;
                screenShareButton.classList.add('active');
                statusDot.classList.add('active');
                statusText.textContent = 'Screen sharing active';

                // Listen for when the user stops sharing via the browser's built-in UI
                stream.getVideoTracks()[0].addEventListener('ended', () => {
                    stopScreenShare();
                });

            } catch (err) {
                console.error("Error accessing the screen: ", err);
                stopScreenShare();
            }
        }

        // Function to stop screen capture
        function stopScreenShare() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                video.srcObject = null;
            }
            isScreenSharing = false;
            screenShareButton.classList.remove('active');
            statusDot.classList.remove('active');
            statusText.textContent = 'Screen sharing inactive';
            stream = null;
        }

        // Screen share button click handler
        screenShareButton.addEventListener('click', () => {
            if (!isScreenSharing) {
                startScreenShare();
            } else {
                stopScreenShare();
            }
        });

        // Function to capture an image from the shared screen
        function captureImage() {
            if (stream && video.videoWidth > 0 && video.videoHeight > 0 && context) {
                canvas.width = 640;
                canvas.height = 480;
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageData = canvas.toDataURL("image/jpeg").split(",")[1].trim();
                currentFrameB64 = imageData;
            }
            else {
                console.log("no stream or video metadata not loaded");
            }
        }

        window.addEventListener("load", async () => {
            // Initialize audio context right away
            await initializeAudioContext();
            connect();
        });

        function connect() {
            console.log("connecting: ", URL);

            webSocket = new WebSocket(URL);

            webSocket.onclose = (event) => {
                console.log("websocket closed: ", event);
                alert("Connection closed");
            };

            webSocket.onerror = (event) => {
                console.log("websocket error: ", event);
            };

            webSocket.onopen = (event) => {
                console.log("websocket open: ", event);
                sendInitialSetupMessage();
            };

            webSocket.onmessage = receiveMessage;
        }

        function sendInitialSetupMessage() {

            console.log("sending setup message");
            setup_client_message = {
                setup: {
                    generation_config: { response_modalities: ["AUDIO"] },
                },
            };

            webSocket.send(JSON.stringify(setup_client_message));
        }


        function sendVoiceMessage(b64PCM) {
            if (webSocket == null) {
                console.log("websocket not initialized");
                return;
            }

            payload = {
                realtime_input: {
                    media_chunks: [{
                        mime_type: "audio/pcm",
                        data: b64PCM,
                    },
                    {
                        mime_type: "image/jpeg",
                        data: currentFrameB64,
                    },
                    ],
                },
            };

            webSocket.send(JSON.stringify(payload));
            console.log("sent: ", payload);
        }

        function receiveMessage(event) {
            const messageData = JSON.parse(event.data);
            const response = new Response(messageData);

            if (response.text) {
                displayMessage("GEMINI: " + response.text);
            }
            if (response.audioData) {
                injestAudioChuckToPlay(response.audioData);
            }
        }


        async function initializeAudioContext() {
            if (initialized) return;

            audioInputContext = new (window.AudioContext ||
                window.webkitAudioContext)({
                sampleRate: 24000
            });
            await audioInputContext.audioWorklet.addModule("pcm-processor.js");
            workletNode = new AudioWorkletNode(audioInputContext, "pcm-processor");
            workletNode.connect(audioInputContext.destination);
            initialized = true;
        }


        function base64ToArrayBuffer(base64) {
            const binaryString = window.atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        function convertPCM16LEToFloat32(pcmData) {
            const inputArray = new Int16Array(pcmData);
            const float32Array = new Float32Array(inputArray.length);

            for (let i = 0; i < inputArray.length; i++) {
                float32Array[i] = inputArray[i] / 32768;
            }

            return float32Array;
        }


        async function injestAudioChuckToPlay(base64AudioChunk) {
            try {
                if (audioInputContext.state === "suspended") {
                    await audioInputContext.resume();
                }
                const arrayBuffer = base64ToArrayBuffer(base64AudioChunk);
                const float32Data = convertPCM16LEToFloat32(arrayBuffer);

                workletNode.port.postMessage(float32Data);
            } catch (error) {
                console.error("Error processing audio chunk:", error);
            }
        }


        function recordChunk() {
            const buffer = new ArrayBuffer(pcmData.length * 2);
            const view = new DataView(buffer);
            pcmData.forEach((value, index) => {
                view.setInt16(index * 2, value, true);
            });

            const base64 = btoa(
                String.fromCharCode.apply(null, new Uint8Array(buffer))
            );

            sendVoiceMessage(base64);
            pcmData = [];
        }

        async function startAudioInput() {
            try {
                // Clean up existing resources first
                await stopAudioInput();
                
                // Get new audio stream
                audioStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 16000,
                    },
                });

                audioContext = new AudioContext({
                    sampleRate: 16000,
                });

                const source = audioContext.createMediaStreamSource(audioStream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);

                processor.onaudioprocess = (e) => {
                    const inputData = e.inputBuffer.getChannelData(0);
                    const pcm16 = new Int16Array(inputData.length);
                    for (let i = 0; i < inputData.length; i++) {
                        pcm16[i] = inputData[i] * 0x7fff;
                    }
                    pcmData.push(...pcm16);
                };

                source.connect(processor);
                processor.connect(audioContext.destination);

                interval = setInterval(recordChunk, 3000);
                
                // Update UI
                isMicActive = true;
                micButton.classList.add('active');
                micButton.innerHTML = '<i class="material-icons">mic</i>';
                micDot.classList.add('active');
                micStatusText.textContent = 'Microphone active';

            } catch (error) {
                console.error("Error starting audio input:", error);
                await stopAudioInput();
            }
        }

        async function stopAudioInput() {
            // Stop the media stream tracks
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
                audioStream = null;
            }

            // Clean up processor
            if (processor) {
                processor.disconnect();
                processor = null;
            }

            // Clean up audio context
            if (audioContext) {
                await audioContext.close();
                audioContext = null;
            }

            // Clear recording interval
            if (interval) {
                clearInterval(interval);
                interval = null;
            }

            // Clear PCM data
            pcmData = [];
            
            // Update UI
            isMicActive = false;
            micButton.classList.remove('active');
            micButton.innerHTML = '<i class="material-icons">mic_off</i>';
            micDot.classList.remove('active');
            micStatusText.textContent = 'Microphone inactive';
        }

        // Mic button click handler
        micButton.addEventListener('click', async () => {
            if (!isMicActive) {
                await startAudioInput();
            } else {
                await stopAudioInput();
            }
        });

        function displayMessage(message) {
            console.log(message);
            addParagraphToDiv("chatLog", message);
        }

        function addParagraphToDiv(divId, text) {
            const newParagraph = document.createElement("p");
            newParagraph.textContent = text;
            const div = document.getElementById(divId);
            div.appendChild(newParagraph);
        }

        class Response {
            constructor(data) {
                this.text = null;
                this.audioData = null;
                this.endOfTurn = null;

                if (data.text) {
                    this.text = data.text
                }

                if (data.audio) {
                    this.audioData = data.audio;
                }
            }
        }

        // Replace the chat section HTML with demo info
        const demoInfoHTML = `
            <div class="demo-info">
                <h2 class="section-title">Demo Controls</h2>
                <div class="info-card">
                    <h3>Screen Share</h3>
                    <p>Click the screen share button to start sharing your screen. The assistant will analyze the content and respond to your voice commands.</p>
                </div>
                <div class="info-card">
                    <h3>Voice Control</h3>
                    <p>Use the microphone button to toggle voice input. When active, speak naturally to interact with the assistant.</p>
                </div>
                <div class="info-card">
                    <h3>Current Status</h3>
                    <div class="status-grid">
                        <div class="status-item">
                            <span class="status-label">Screen Share:</span>
                            <span class="status-value" id="screenStatus">Inactive</span>
                        </div>
                        <div class="status-item">
                            <span class="status-label">Microphone:</span>
                            <span class="status-value" id="micStatus">Inactive</span>
                        </div>
                        <div class="status-item">
                            <span class="status-label">Connection:</span>
                            <span class="status-value" id="connectionStatus">Disconnected</span>
                        </div>
                    </div>
                </div>
            </div>
        `;

        // Replace the chat section with demo info
        document.querySelector('.chat-section').innerHTML = demoInfoHTML;

        // Update status indicators
        function updateStatusIndicators() {
            document.getElementById('screenStatus').textContent = isScreenSharing ? 'Active' : 'Inactive';
            document.getElementById('micStatus').textContent = isMicActive ? 'Active' : 'Inactive';
            document.getElementById('connectionStatus').textContent = webSocket && webSocket.readyState === WebSocket.OPEN ? 'Connected' : 'Disconnected';
        }

        // Update status indicators periodically
        setInterval(updateStatusIndicators, 1000);
    </script>

</body>

</html></html>
